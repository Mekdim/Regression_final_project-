---
title: "Stat 615 Final_Project"
author: "Dhruv Jain & mekdim"
date: "2023-04-20"
output:
  pdf_document: default
  word_document: default
  html_document: default
linkcolor: blue
number_sections: true
toc_float: true
theme: cerulean
---

***
\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## ACKNOWLEDGEMENTS

"I am not what happened to me, I am what I choose to become"
by Christopher Gardner, The Pursuit of Happiness.

It is always a pleasure to remember the fine people who guided me in the Regression program. I received to uphold my practical and theoretical skills during the respective session. Firstly, I would like to thank **Pro. James C. Dickens** and secondly, I want to thank my family & friends for their love, motivation, and support during this semester in American university. Thanks for all the ideas, opinions, knowledge, and suggestions given to me to help me to complete this report. We are very thankful to American University for giving us the opportunity to pursue this project.

# Title Page with Executive Summary

**Title:** Estimating Medical Cost.

**Type of analysis:** Application analysis

**Table 1:**
<table>

Name                                   course
-------------                        ---------
Dhruv Jain                            STAT -615
Mekdim Ashebo                         STAT -615

</table>


***
\newpage

```{r,warning=FALSE,message=FALSE }
# calling all the libraries used in the code book 
library(olsrr)
library(tidyverse)
library(dbplyr)
library(dplyr)
library(Matrix)
library(MASS)
library(ggplot2)
library(tibble)
library(data.table)
library(ggmosaic)
library(ggforce)
library(ggmap)
library(ggthemes)
library(purrr)
library(keep)
library(readr)
library(gridExtra)
library(randomForest)
library(corrplot)
library(PerformanceAnalytics)

```

# 1 

## 1.1 About Data set 
```{r}
# offer a preliminary description of the data set. For example, indicate the size of the data source, describe the variables, and include any other data profile information that would be of interest.

#data set source: https://www.kaggle.com/datasets/mirichoi0218/insurance

# Columns Description

#age: age of primary beneficiary
#sex: insurance contractor gender, female, male
#bmi: Body mass index, providing an understanding of body, weights that are relatively high or low relative to height,
#objective index of body weight (kg / m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9
#children: Number of children covered by health insurance / Number of dependents
#smoker: Smoking
#region: the beneficiary's residential area in the US, northeast, southeast, southwest, northwest.
#charges: Individual medical costs billed by health insurance


# We had to randomly sample 300 rows from our original data. 
# We then saved these 300 rows into csv file so that we can import them later.
# going forward We will take that csv file. (which has only be run once)

# The preliminary steps we did 
#insurance <- read_csv('Downloads/insurance.csv')
#insurance_300 <- sample_n(insurance, 300)

#write.csv(insurance_300 , file = "Desktop/insurance_300.csv")

# Let import the 300 rows 

# insurance_new <- read_csv("insurance_300.csv")

# Read in CSV file and specify column types
insurance_new <- read_csv("insurance_300.csv", 
col_types = cols(
  age = col_double(),
  sex = col_character(),
  bmi = col_double(),
  children = col_double(),
  smoker = col_character(),
  region = col_character(),
  charges = col_double()
))
#300 rows  and 7 columns 
# This project is about determining the factors that affect medical costs billed by health insurance 
# The independent variables include three categorical variables and three quantitative variables. 
# Sex, region(Northeast, northwest etc), and smoker(whether a person smokes or not) are the categorical 
# variables. While the quantitative variables include the BMI index, the age and the number of children the person have. 

nrow(insurance_new)
ncol(insurance_new)

# Let us quickly investigate the summary of our dependent variable 
#  The median insurance charge is around 10097 and the mean of 13283. The 
# standard deviation is 11399.

summary(insurance_new$charges)
sd(insurance_new$charges)
head(insurance_new,10)
```

## 1.2  cleaning the data and type of columns 

```{r}
# calling the data set using read csv file
# insurance_new <- read_csv('insurance_300.csv')
# number of rows in data 
nrow(insurance_new)
# number of colums in data set 
ncol(insurance_new)
# colums names 
colnames(insurance_new)
# visual data set look like 
head(insurance_new,10)
# type of columns used in data frame (double, charterer)
str(insurance_new)

# summary of data colum wise 
summary(insurance_new)
# calculating NA/missing data in columns 
colSums(is.na(insurance_new))

# converting to factor variable 
insurance_new$sex = as.factor(insurance_new$sex)
insurance_new$smoker = as.factor(insurance_new$smoker)
# how many unique values 
unique(insurance_new$sex)
unique(insurance_new$children)
unique(insurance_new$smoker)
unique(insurance_new$region)
# Check levels of smoker variable
table(insurance_new$smoker)
# Check levels of region variable
table(insurance_new$region)
# Check levels of sex variable
table(insurance_new$sex)

```

## 1.3 visualization

```{r}
# Does age affect medical charges for smoker? 
ggplot(data = insurance_new , 
       aes(x=age, y=charges,shape=smoker,color = smoker)) +
  geom_point()+
  geom_smooth(method=lm)
# Yes the charges are increased as we increase the number of age. Now the fun part is if a person smokes he/she is paying more charges on medical then the person not smoking.

# Does Body mass index (BMI) affect medical charges for smoker? 
ggplot(data = insurance_new , 
       aes(x=bmi, y=charges,shape=smoker,color = smoker)) +
  geom_point()+
  geom_smooth(method=lm)
# One can clearly observe that smoking affect in BMI and increased  with the medical expenses. 


# Histogram for density graph for Body mass index (BMI) 
ggplot(data = insurance_new , aes(x=bmi,color=sex)) +
 geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666") 


# box plot 
# comparing that does region of living affects the smokers or not. Now, looking at graph one cans say that the region does affect the charges on medical insurance. 

ggplot(data = insurance_new , 
       aes(x=region, y=charges,shape=smoker,fill = smoker)) +
  geom_boxplot(color="black")+
  geom_smooth(method=lm)+
  theme_classic()+
  theme(legend.position="bottom")+
  geom_jitter(shape=16, position=position_jitter(0.2))


# How many children per male/female.
ggplot(data = insurance_new , aes(x=children,color=sex)) +
 geom_histogram()
```


## ???? need to see what can be done.
```{r}
insurance_new%>%
  filter(sex == "female")%>%
  count(sex,children,smoker,region)%>%
  arrange(sex, smoker)
  


insurance_new  
ab <- insurance_new%>%
  dplyr::select(age,smoker,sex)
ab


```



# 2  

## 2.1 multicollinearity plot 

The response variable is not dependent on explanatory variable interns of multicollinearity.

The highest correlation is between charges and age with only 0.24. But if we exclude charges  since charges is dependent variable, the highest correlation among the independent variables.

The age with bmi with only 0.04 which is nearly 0. So there exists no colinearity among the independent variables. This suggests that each of the variables might be useful if they are included in the regression model as they dont have any correlation with each other. 


```{r}
numeric_insurance <- cor(insurance_new[,c("bmi", "children", "age", "charges")])

numeric_insurance
```

we can also the scatter plots between the independent variables clearly there is no pattern that we can see verifying our output from the correlation matrix. 

One can say from the graph that the points are independently plotted and one cannot find any kind of pattern on left side of graph. On the other hand one can identify the  

```{r fig.cap= "multicollinearity plot", warning=FALSE, echo=FALSE}

chart.Correlation(numeric_insurance, histogram=TRUE, pch=19)

```


## 2.2 Normality plot

```{r fig.cap= "Q-Q plot"}
# Light tailed at the end 
qqnorm(insurance_new$bmi)
# right skewed 
qqnorm(insurance_new$charges)
# Heavy tailed at the end 
qqnorm(insurance_new$age)

```



# 3 

## 3.1 full regression model 
## Make use of the R generated dummy variable matrices ??

full regression model including both categorical and quantitative variables 

```{r}
lm(charges ~ age + children + bmi + region + sex + smoker,data = insurance_new) -> x
x
```

# 4

## 4.1  Fitted & residual values using matrix for quantitative variables 

```{r}
Xm <-  model.matrix(~age + children + bmi   , data=insurance_new )
Xm
Ym <- as.matrix(insurance_new%>%dplyr::select(charges))
Ym
# Let's use R code to establish matrix X :
#------------------------------------------------------------------- 
#  A = (X^T*X)^-1*X^TY 
(solve(t(Xm)%*%Xm))%*%(t(Xm)%*%Ym)
#-------------------------------------------------------------------
# fitted values 
Xm%*%((solve(t(Xm)%*%Xm))%*%(t(Xm)%*%Ym)) -> fitted_values
#-------------------------------------------------------------------
# residual values 
Ym-Xm%*%((solve(t(Xm)%*%Xm))%*%(t(Xm)%*%Ym)) -> residual_values
#-------------------------------------------------------------------
# producing the table of residula and fitted plot for the model.
matrix = data.frame(fitted_values, residual_values)
names(matrix)[1] <- "fitted_values"
names(matrix)[2] <- "residual_values"
matrix

#-------------------------------------------------------------------
# Both fitted values and residual values match with matrix model  
lm(charges ~ age + children + bmi, data= insurance_new) -> AB
summary(AB)
residuals(AB)
fitted(AB)
```

## 4.2  Matrix method with both quantitative and qualitative variables(dummy variables included automatically)

```{r}

# The results are the same using both the matrix method and lm method. 

Xm <-  model.matrix(~age + children + bmi + region + sex+ smoker  , data=insurance_new )
Ym <- as.matrix(insurance_new%>%dplyr::select(charges))

# Let's use R code to establish matrix X :
#------------------------------------------------------------------- 
#  A = (X^T*X)^-1*X^TY 
(solve(t(Xm)%*%Xm))%*%(t(Xm)%*%Ym)
#-------------------------------------------------------------------
# fitted values 
Xm%*%((solve(t(Xm)%*%Xm))%*%(t(Xm)%*%Ym)) -> fitted_values
#-------------------------------------------------------------------
# residual values 
Ym-Xm%*%((solve(t(Xm)%*%Xm))%*%(t(Xm)%*%Ym)) -> residual_values
#-------------------------------------------------------------------
# producing the table of residula and fitted plot for the model.
matrix = data.frame(fitted_values, residual_values)
names(matrix)[1] <- "fitted_values"
names(matrix)[2] <- "residual_values"
matrix

#-------------------------------------------------------------------
# Both fitted values and residual values match with matrix model  
lm(charges ~ age + children + bmi+ region + sex + smoker, data= insurance_new) -> AB
summary(AB)
residuals(AB)
fitted(AB)
```



# 5 Analyze and evaluate the full model

```{r}
lm(charges ~ age + children + bmi+ region + sex + smoker, data= insurance_new) -> AB
summary(AB)

```
## 5.1 Coefficients:

### charges =  -12033.25 + (age)*261.14 + (children)*532.76 + (bmi)*353.35 + (region_northwest)(-1545.53) + (region_southeast)*(-1505.35) + (regionsouthwest)*(-1719.90) + (sexmale)*(607.78) + (smokeryes)*(22876.38)

The **Estimate column** lists the estimated coefficients of the predictor variables included in the model. For instance, the "age" coefficient has an estimated value of 261.14, which means that for every one-unit increase in age, the outcome variable (presumably a medical cost) is estimated to increase by $261.14, holding all other variables constant.

## 5.2  Standard Errors:
The **Std Error** column lists the standard errors of the estimated coefficients. These are measures of the uncertainty or variability in the estimated coefficients. Smaller standard errors indicate more precise estimates.

## 5.3 T-values:
The **t-value** column lists the t-statistics for each coefficient. These values represent the estimated coefficients divided by their standard errors. T-values are used to test the null hypothesis that the true coefficient is zero. Larger t-values indicate a stronger evidence against the null hypothesis.

## 5.4 P-values
The pr column lists the p-values associated with the t-values. P-values represent the probability of observing the t-value or a more extreme value if the true coefficient is zero. Smaller p-values indicate stronger evidence against the null hypothesis. 

The significance codes provided in the table help to quickly identify significant coefficients; for instance, **age,bmi,smokers** represents p-value less than two decimal places and **children** represents p-value less than 0.05.One can say that **region** represents bigger p-value than significant level. 

## 5.5 Residual Standard Error:
The **Residual standard error** provides an estimate of the variability of the errors or unexplained variance in the model. It measures the average distance that the observed values fall from the predicted values.

## 5.6 R-squared:
The "Multiple R-squared" and "Adjusted R-squared" measures how well the model fits the data. R-squared ranges from 0 to 1 and represents the proportion of variance in the outcome variable that is explained by the predictor variables. Adjusted R-squared is a corrected version of R-squared that takes into account the number of predictor variables in the model. In full model one can see 0.7379.

## 5.7 summary   

This data summary provides information on the estimated coefficients, their standard errors, t-values, and corresponding p-values, as well as model diagnostics such as residual standard error, multiple R-squared, adjusted R-squared, and F-statistic. These measures can be used to interpret the strength and significance of the relationship between the predictor variables and the outcome variable, as well as the overall goodness-of-fit of the model.
Still working to find the best parameters to be included in the model.   


#  6 confidence intervals for all variables used in full model

```{r}
confint(AB,level = 0.95)
```

# 7  Now produce a reduced model (removing variables of your choice with justification). Use R summary coding for both models and offer justification for choosing one model over the othe



# 7.1  Evaluating Various regression models. 

Let us begin by using a multiple linear regression model that uses all the six variables. From the summary table we see that our R squared and Adjusted r square are around 0.73 and the  residual standard error is 5915.  The r squared value is high enough to be considered good but let us continue finding better fits. 
```{r}
ols_step_all_possible(AB) ->  allmodels
as_tibble(allmodels) -> allmodels_1
tail(allmodels_1,8)
# the best model is represented by including all age, children,bmi, region, smoker full model.
#plot(allmodels)
```
